## 祖龙娱乐   sp一面   时长 73分钟  (包括答案）精
### author 
来我海康大家庭
### post-time 

编辑于  2019-07-26 11:26:30
### content 
<div class="post-topic-des nc-post-content">
 <h3>
  面试大数据研发工程师
 </h3>
 <h3>
  <span>
   在电脑前苦等半小时，终于面试官上线了，一看，一下两个人，我去，心理压力好大，赶紧放松放松心情
  </span>
 </h3>
 <p>
  先简单的来个自我介绍：
 </p>
 <p>
  <strong>
   1  自我介绍
  </strong>
 </p>
 <p>
  <strong>
   2  你自己搭过大数据集群吗？ 那你说说搭建hadoop集群的3个xml文件
  </strong>
 </p>
 <ul>
  <li>
   core-site.xml
  </li>
  <li>
   hdfs-site.xml
  </li>
  <li>
   mapred-site.xml   心想这简单，赶紧将3个xml说出来，并简单说了下里面都包括啥
  </li>
 </ul>
 <p>
  <strong>
   3 正常的hadoop集群工作都会启动哪些进程？
  </strong>
 </p>
 <p>
  当时回答是：namenode,datanode ,secondarynode ,
 </p>
 <p>
  <strong>
   4.他们的作用分别是什么？
  </strong>
 </p>
 <ul>
  <li>
   nameNode   主节点 负责维护整个Hdfs文件系统的目录树，以及每个文件所对应的block块信息(元数据)
  </li>
  <li>
   DataNode  从节点  负责存储具体的文件数据，并且每个block可以在多个datanode上存储多个副本
  </li>
  <li>
   secondary  nameNode  相当于一个备用的naneNode， 当nameNode死机之后，可以将secondary nameNode
  </li>
  <li>
   的数据备份到nameNode上面 ，但不能备份完整数据，它有两大功能，1 镜像备份，2 日志与镜像定期合并
  </li>
 </ul>
 <p>
  <strong>
   5 你能详细介绍一下secondaryNode 的具体作用吗？
  </strong>
 </p>
 <p>
  当然可以，这个问题我可是仔细研究过，哈哈
 </p>
 <ul>
  <li>
   secondary  nameNode会经常向namenode发送请求，是否满足check。
  </li>
  <li>
   当条件满足时，secondary  nameNode将进行checkPoint  。
  </li>
  <li>
   这时nameNode 滚动当前正在写的edits，将刚刚滚动掉的和之前edits文件进行合并。
  </li>
  <li>
   secondary  nameNode下载edis文件，然后将edits文件和自身保存的fsimage文件在内存中进行合并，
  </li>
  <li>
   然后写入磁盘并上传新的fsimage到nameNode，这时nameNode将旧的fsimage用新的替换掉。
  </li>
 </ul>
 <p>
  <strong>
   6  看来你掌握的还不错啊，HDFS的块默认是保存几份？一个块多大？
  </strong>
 </p>
 <p>
  默认保存是3份，一个块是128M。
 </p>
 <p>
  <strong>
   7 之前的64M 是从哪个版本变换的？
  </strong>
 </p>
 <p>
  hadoop 1.0  默认是64M， hadoop 2.0 由64M 改为128M
 </p>
 <p>
  <strong>
   8 那假设现在是128M，那我在工作中想把它调为256M，那我需要调整什么，才能改变块的大小？
  </strong>
 </p>
 <p>
  主要是磁盘的存储决定 块的大小，块组成的文件的大小取决于磁盘的传输速率，调整磁盘，可以改变块的大小。
 </p>
 <p>
  <strong>
   9 Hdfs的读写过程你了解吗？ 简单讲讲？
  </strong>
 </p>
 <p>
  那我就说说写过程吧，
 </p>
 <ul>
  <li>
   1、客户端跟nameNode 通信，请求上传文件，nameNode检查文件，父目录是否存在，并向客户端返回是否可以上传文件
  </li>
  <li>
   2、客户端请求第一个block块该上传到哪个datanode服务器上，nameNode查询从节点之后，返回对应的danaNode 服务器
  </li>
  <li>
   A,B,C等。
  </li>
  <li>
   3、客户端请求nameNode服务器，采取就近原则，选择A服务器上传数据(本质上是个RPC调用，建立PipeLine)，A收到
  </li>
  <li>
   请求后，A调B，B调C，将每个pipline建立连接，然后逐级返回给客户端
  </li>
  <li>
   4 客户端开始往A上传第一个block，以Package为单位，A收到一个Package，就会传给B，B传给C，A每传一个package就会
  </li>
  <li>
   放入一个应答队列，等待应答。
  </li>
  <li>
   5、当第一个block传输完成后，客户端再次请求namenode上传第二个block。
  </li>
 </ul>
 <p>
  <strong>
   10 挺好，那你说一下MapReduce的工作原理？
  </strong>
 </p>
 <ul>
  <li>
   1、客户端启动一个job,然后向jobTracker请求一个jobID
  </li>
  <li>
   2、 然后将运行所需要的资源文件上传到HDFS上，包括Mapreduce程序打包的jar包，配置文件，以及计算的输入划分信息等
  </li>
  <li>
   3、 这些文件全部存储在JobTracker专门创建的jobID文件夹中（jar文件会有10个副本，输入划分信息对应着jobTracker应
  </li>
  <li>
   该启动多少个Map任务）
  </li>
  <li>
   4、JobTracker将这些资源文件放入作业队列中，调度器根据调度算法对作业文件进行调度，根据输入划分信息划分Map任务
  </li>
  <li>
   并将map任务分配给TaskTracker执行。
  </li>
  <li>
   5、taskTracker每隔一段时间发送给jobTracker一个心跳，告诉它自己的运行情况，这个心跳中包含map任务完成的进度等。
  </li>
  <li>
   6.当最后一个任务完成后，jobTracker会将该任务设为成功，返回给客户端。客户端得到结果，得知任务完成便显示
  </li>
  <li>
   消息给用户。
  </li>
 </ul>
 <p>
  <strong>
   11 你在具体讲一下map中的一些步骤，例如partition,sort,combiner,shuffle等等。
  </strong>
 </p>
 <p>
  好的，sort 主要是排序，combiner是合并，partition是分片等，
 </p>
 <p>
  首先Mapper根据文件进行分区，sort将Mapper产生的结果按照key进行排序，combiner将key相同的记录进行
 </p>
 <p>
  合并，partition是吧数据均衡的分配个Reducer. shuffle是Mapper将结果传给Reduce，在这期间容易发生数据倾斜等。
 </p>
 <p>
  <strong>
   12 那这个数据倾斜一般是在Mapper端发生的还是Reduce中发生的？
  </strong>
 </p>
 <p>
  Mapper将数据处理完传给Reduce，当Reduce进行处理时，因为一部分key的数据量过大，导致其他分区已经执行完成
 </p>
 <p>
  而数据量过大的key执行时间过长，所以数据倾斜是发生在Reduce端的。
 </p>
 <p>
  <strong>
   13，对，那发生数据倾斜是因为这个key分布不均匀，那你会怎么优化呢？
  </strong>
 </p>
 <p>
  因为研究生期间研究的课题就是关于Spark的并行大数据清洗，所以对MapReduce和Spark发生数据倾斜的过程和解决方法
 </p>
 <p>
  比较熟悉，
 </p>
 <p>
  可以在Mapper期间将大数据量相同的key进行分散，通过添加N以内的随机数前缀，对数据较多的Key进行子扩展，
 </p>
 <p>
  先进行局部操作，再去除随机数之后进行聚合操作，避免在进行Shuffle操作时出现数据倾斜问题。
 </p>
 <p>
  <strong>
   14 那Mapper端进行combiner之后，除了速度会提升，那从Mapper端到Reduece端的数据量会怎么变？
  </strong>
 </p>
 <p>
  数据量会减少，因为combiner之后，会将相同的key进行一次聚合，数据量会在这时候减少一部分
 </p>
 <p>
  <strong>
   15 map 输出的数据如何超出他的那个小文件内存之后，那他是落地到磁盘还是落地到HDFS中？
  </strong>
 </p>
 <p>
  落地到磁盘中，因为map,reduce操作，就是一次次的I/O请求
 </p>
 <p>
  <strong>
   16  Map到Reduce默认的分区机制是什么？
  </strong>
 </p>
 <p>
  这个是根据那个hash进行计算   对map中的key做hash，对reduce个数取模
 </p>
 <p>
  <strong>
   17 hadoop的调优主要针对配置文件的调优你知道哪几种？
  </strong>
 </p>
 <ul>
  <li>
   思考了一下，
  </li>
  <li>
   1、因为Mapreduce运算时是在磁盘中进行的，所以 通过修改磁盘I/O，也就是设置和的预读缓冲区大小
  </li>
  <li>
   来提高hadoop里面大文件顺序读的性能。以此来提高I/O性能。
  </li>
  <li>
   2、通过修改三个配置文件的参数如 core-site.xml,mapred-site.xml,hdfs-site.xml等
  </li>
  <li>
   例如 修改core 文件里面的buffer.size，来修改读写缓冲区的大小，还有hdfs文件里面的block.size修改块的大小等
  </li>
  <li>
   都可以进行调优
  </li>
 </ul>
 <p>
  <strong>
   18 好的，给你出个题，现在有1G的数据文件，里面有四个字段，分别是id,name,age,class,然后要按照class来分组，
  </strong>
 </p>
 <p>
  <strong>
   id来排序，口述一下mapreduce的过程是怎么实现的？这里面会有几个map?
  </strong>
 </p>
 <p>
  思考了一下，
 </p>
 <ul>
  <li>
   1、首先1G文件，那默认一个块是128M，所以可以分为8个块，对应的就是8个Mapper
  </li>
  <li>
   2、然后定义一个对象，将四个属性封装到对象中，实现序列化和反序列化
  </li>
  <li>
   3、定义一个类继承partitioner类，调用对象中的class属性设置分组，
  </li>
  <li>
   4  在map端对文件进行读取，然后通过Split来进行分割，调用对象的id作为key，然后进行局部sort排序，在combiner局部聚合
  </li>
  <li>
   后通过reduce来进行整体聚合。
  </li>
 </ul>
 <p>
  说完之后感觉对着吧，果然，听见面试官说嗯嗯，好。觉得差不多对啦
 </p>
 <p>
  <strong>
   19 嗯嗯，好，说说yarn吧，它有什么优势，能解决什么问题？
  </strong>
 </p>
 <p>
  yarn集群主要分为主节点ResourceManage,从节点 NodeManage  ResourceManage负责资源的分配，将集群的资源分配给
 </p>
 <p>
  各个应用使用，资源分配的基本单元是Container，NodeManage则是一个计算节点的管理者，负责启动应用的
 </p>
 <p>
  所需的Conbiner，并对内部资源进行监控等。
 </p>
 <p>
  yarn一般和mapreduce进行结合，主要是对mapreduce中的资源计算进行维护等。
 </p>
 <p>
  答完之后，心想别问yarn吧，这块看得不是很深，哈哈，果然，面试官问了一个问题后就跳过了
 </p>
 <p>
  <strong>
   20 说说Spark吧，Spark为啥比Mapreduce运行块，原因都有哪些?
  </strong>
 </p>
 <p>
  1 spark是基于内存计算，mapreduce是基于磁盘运算，所以速度快
 </p>
 <p>
  2 spark拥有高效的调度算法，是基于DAG,形成一系列的有向无环图
 </p>
 <p>
  3 spark 是通过RDD算子来运算的，它拥有两种操作，一种转换操作，一种动作操作，可以将先运算的结果存储在
 </p>
 <p>
  内存中，随后在计算出来
 </p>
 <p>
  4 spark 还拥有容错机制Linage
 </p>
 <p>
  <strong>
   21 什么是RDD？
  </strong>
 </p>
 <p>
  RDD就是弹性分布式数据集，可以理解为一种数据结构，拥有多种不同的RDD算子
 </p>
 <p>
  <strong>
   22 你都知道哪些RDD算子？
  </strong>
 </p>
 <p>
  比如转换操作，有map().fliter() flatMap()，distinct()等  动作操作  有 collect ,reduce 等
 </p>
 <p>
  <strong>
   23. 你知道reduceBykey 和groupBykey有啥区别吗？
  </strong>
 </p>
 <p>
  reduceByKey会在结果发送至reducer之前会对每个mapper在本地进行merge，
 </p>
 <p>
  有点类似于在MapReduce中的combiner。这样做的好处在于，在map端进行一次reduce之后，数据量会大幅度减小，
 </p>
 <p>
  从而减小传输，保证reduce端能够更快的进行结果计算。
 </p>
 <p>
  groupByKey会对每一个RDD中的value值进行聚合形成一个序列(Iterator)，此操作发生在reduce端，
 </p>
 <p>
  所以势必会将所有的数据通过网络进行传输，造成不必要的浪费。同时如果数据量十分大，
 </p>
 <p>
  可能还会造成OutOfMemoryError。
 </p>
 <p>
  <strong>
   24.现在有一个业务，当SparkStreaming在消费kafka里面的数据，然后消费了一段时间之后，程序挂了，当
  </strong>
 </p>
 <p>
  <strong>
   下一次程序启动时如何保证SparkStraming能继续消费kafka之前的位置?
  </strong>
 </p>
 <p>
  听到这个问题时，我就偷笑啦，幸亏上次海康威视问过我，我就好好看了一下
 </p>
 <p>
  可以依靠checkPoint机制来保证，每次SparkStreaming消费kafka数据后，将消费的kafka offsets更新到checkpoint，当
 </p>
 <p>
  程序挂机或升级时，就可以用过读取checkpoint 的记录来接着上次的位置进行读取，实现数据的零丢失。
 </p>
 <p>
  <strong>
   25，除了这种方式还有什么方式？
  </strong>
 </p>
 <p>
  还可以在sparkStreaming中另外启动一个预写日志，这将同步保存所有收到的kafka数据导hdfs中，以便发生故障时，
 </p>
 <p>
  恢复到上次的位置和之前的数据。
 </p>
 <p>
  <strong>
   26，你说说Spark的广播变量？
  </strong>
 </p>
 <p>
  听到这个问题后，一脸懵逼，不会拉。。 我都猜想 面试官肯定在想，小样，我还难不倒你拉。。。。
 </p>
 <p>
  然后我就让面试官给我讲了一下。。
 </p>
 <p>
  Spark中因为算子中的真正逻辑是发送到Executor中去运行的，所以当Executor中需要引用外部变量时，
 </p>
 <p>
  需要使用广播变量。广播变量只能在Driver端定义，不能在Executor端定义，在Driver端可以修改广播
 </p>
 <p>
  变量的值，在Executor端无法修改广播变量的值
 </p>
 <p>
  <strong>
   27 那你知道累加器吗？
  </strong>
 </p>
 <p>
  之前看过一点，累机器相当于统筹大变量，常用于计数，统计。累加器常常被作为rdd的map filter操作的副产品等。
 </p>
 <p>
  <strong>
   28.你说说spark中 job,stage,task，分别代表什么？
  </strong>
 </p>
 <p>
  Job简单讲就是提交给spark的任务。 Stage是每一个job处理过程要分为的几个阶段。
 </p>
 <p>
  Task是每一个job处理过程要分几为几次任务。Task是任务运行的最小单位。最终是要以task为单位运行在executor中。
 </p>
 <p>
  <strong>
   29.嗯嗯 好，说说Spark的工作机制？
  </strong>
 </p>
 <p>
  我去，咋问的都是大问题啊，幸亏之前复习过。。
 </p>
 <p>
  用户在客户端提交job作业后，会由driver运行main方法并创建SparkContext上下文。执行RDD算子，形成DAG图，
 </p>
 <p>
  然后将DAG图交给DAGScheduler来处理。DAGScheduler按照RDD之间的依赖关系划分stage，输入task Scheduler，
 </p>
 <p>
  task Scheduler会将stage划分为task set分发到各个节点的executer中执行，executor以多线程的方式执行，每个线程
 </p>
 <p>
  负责一个任务，任务结束后，根据不同类型的任务返回不同的结果。
 </p>
 <p>
  <strong>
   30  你了解zookeeper吗？
  </strong>
 </p>
 <p>
  zookeeper 是一个分布式协调服务，zookeeper集群包括 leader 和 follow
 </p>
 <p>
  31
  <strong>
   说说zookeeper的***过程，比如现在有五台机器，ABCDE依次启动起来，那么哪台是leader？
  </strong>
 </p>
 <p>
  记得不太清楚了。。就大概说了一下
 </p>
 <p>
  1.首先更新logicalclock并提议自己为leader并广播出去
 </p>
 <p>
  2.进入本轮投票的循环
 </p>
 <p>
  3.从recvqueue队列中获取一个投票信息，如果为空则检查是否要重发自己的投票或者重连，否则
 </p>
 <p>
  判断投票信息中的***状态： 就回答到这，后来下来百度了一下。。。
 </p>
 <p>
  <strong>
   32 hive了解吗？
  </strong>
 </p>
 <p>
  Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能
 </p>
 <p>
  <strong>
   33.说说内部表和外部表的区别？
  </strong>
 </p>
 <p>
  内部表的数据是由Hive自身管理的，外部表的数据是由HDFS管理的；
 </p>
 <p>
  删除内部表会删除元数据和存储的数据；删除外部表只删除元数据不删除存储的数据
 </p>
 <p>
  <strong>
   34,你知道UDF吗？
  </strong>
 </p>
 <p>
  UDF就是Hive提供的内置函数无法满足业务处理需要时，可以考虑使用用户自定义函数。
 </p>
 <p>
  <strong>
   35 一张大表，一张小表，你写join in时，哪个表放左边，哪个表放右边？
  </strong>
 </p>
 <p>
  小表放前，大表放后，左查询，根据小表为主进行查询。
 </p>
 <p>
  <strong>
   36  问一下kafka的问题吧，kafka是怎么进行数据备份的？
  </strong>
 </p>
 <p>
  哇，面试官 你是要把大数据里面的每个组件分别问一下，。。。。深呼一口气，思考了一下 然后巴拉巴拉
 </p>
 <p>
  备份机制是Kafka0.8版本之后出的，一个备份数量为n的集群允许n-1个节点失败。在所有备份节点中，
 </p>
 <p>
  有一个节点作为lead节点，这个节点保存了其它备份节点列表，并维持各个备份间的状体同步。
 </p>
 <p>
  <strong>
   37.消费者是从leader中拿数据，还是从follow中拿数据？
  </strong>
 </p>
 <p>
  。。。不太会，备份机制这块没咋深入了解过。
 </p>
 <p>
  kafka是由follower周期性或者尝试去pull(拉)过来(其实这个过程与consumer消费过程非常相似)，
 </p>
 <p>
  写是都往leader上写，但是读并不是任意flower上读都行，读也只在leader上读，flower只是数据的一个备份，
 </p>
 <p>
  保证leader被挂掉后顶上来，并不往外提供服务。
 </p>
 <p>
  <strong>
   38.那换个问题吧。说说kafka的ISR机制？
  </strong>
 </p>
 <ul>
  <li>
   kafka 为了保证数据的一致性使用了isr 机制，
  </li>
  <li>
   1. leader会维护一个与其基本保持同步的Replica列表，该列表称为ISR(in-sync Replica)，每个Partition都会有一个ISR，
  </li>
  <li>
   而且是由leader动态维护
  </li>
  <li>
   2. 如果一个flower比一个leader落后太多，或者超过一定时间未发起数据复制请求，则leader将其重ISR中移除
  </li>
  <li>
   3. 当ISR中所有Replica都向Leader发送ACK时，leader才commit
  </li>
 </ul>
 <p>
  <strong>
   39.kafka如何保证数据的不重复和不丢失？
  </strong>
 </p>
 <p>
  答案上面已经回到了，面试官又问一遍。。可能是看我kafka这块了解不是很深入。想再虐虐我。。。
 </p>
 <p>
  <strong>
   40.kafka里面存的数据格式都是什么样的？
  </strong>
 </p>
 <p>
  topic主题，然后主题进行分区  topic 分为partition , partition里面包含Message。
 </p>
 <p>
  <strong>
   41.kafka中存的一个是数据文件，一个是索引文件，说说这个？
  </strong>
 </p>
 <p>
  。。。。。不太会。。。哇，kafka被虐惨啦
 </p>
 <p>
  <strong>
   42.kafka 是如何清理过期数据的？
  </strong>
 </p>
 <p>
  kafka的日志实际上是以日志的方式默认保存在/kafka-logs文件夹中的，默认7天清理机制，
 </p>
 <p>
  日志的真正清理时间。当删除的条件满足以后，日志将被“删除”，但是这里的删除其实只是将
 </p>
 <p>
  该日志进行了“delete”标注，文件只是无法被索引到了而已。但是文件本身，仍然是存在的，只有当过了log.segment.delete.delay.ms 这个时间以后，文件才会被真正的从文件系统中删除。
 </p>
 <p>
  <strong>
   43.一条message中包含哪些信息？
  </strong>
 </p>
 <ul>
  <li>
   包含 header,body。
  </li>
  <li>
   一个Kafka的Message由一个固定长度的header和一个变长的消息体body组成。
  </li>
  <li>
   header部分由一个字节的magic(文件格式)和四个字节的CRC32(用于判断body消息体是否正常)构成。
  </li>
  <li>
   当magic的值为1的时候，会在magic和crc32之间多一个字节的数据：attributes(保存一些相关属性，比如是否压缩、
  </li>
  <li>
   压缩格式等等)；
  </li>
  <li>
   如果magic的值为0，那么不存在attributes属性body是由N个字节构成的一个消息体，包含了具体的key/value消息
  </li>
 </ul>
 <p>
  <strong>
   44.嗯，行，你知道mysql的最左原则吗？
  </strong>
 </p>
 <p>
  终于把kafka过去啦。。心累
 </p>
 <p>
  最左原则：顾名思义，就是最左优先，比如现在有一张表，里面建了三个字段ABC,对A进行主键，BC建立索引，就相当于
 </p>
 <p>
  创建了多个索引，A索引，(A,B)组合索引，(A,B，C)组合索引，那查询时，会根据查询最频繁的 放到最左边。
 </p>
 <p>
  <strong>
   嗯 好，我的问题问完了，让我同事问问你。
  </strong>
 </p>
 <p>
  <strong>
   已经问了40分钟纯问题啦，，再换个面试官，好的，可以
  </strong>
 </p>
 <p>
  <strong>
   45，刚才我的同事问的都是大数据相关的，那我们问点java相关的。
  </strong>
 </p>
 <p>
  终于问java啦，下面的java问题每个都回答出来了，就不写答案啦
 </p>
 <p>
  <strong>
   46.说说抽象类和接口？
  </strong>
 </p>
 <p>
  <strong>
   47，集合了解吧，说说集合有几大类,分别介绍一下？
  </strong>
 </p>
 <p>
  <strong>
   48，hashMap顶层实现了解过吗？具体讲讲
  </strong>
 </p>
 <p>
  <strong>
   49，说说hashMap在1.8之后优化的环节
  </strong>
 </p>
 <p>
  <strong>
   50. HashMap 和 hashTable的区别？
  </strong>
 </p>
 <p>
  <strong>
   51.另一个线程安全的是啥？
  </strong>
 </p>
 <p>
  <strong>
   52.说说ConcurrentHashMap的底层实现
  </strong>
 </p>
 <p>
  <strong>
   53.java实现多线程的方式有几种？
  </strong>
 </p>
 <p>
  <strong>
   54.讲讲 synchronized，Lock，ReetrantLock之间的区别
  </strong>
 </p>
 <p>
  <strong>
   55.java的线程大概有几种状态？
  </strong>
 </p>
 <p>
  <strong>
   56.sleep 和 wait方法的区别？
  </strong>
 </p>
 <p>
  <strong>
   57.说说volatile关键字
  </strong>
 </p>
 <p>
  <strong>
   58.说说JVM内存区域分为几大块，分别讲一下
  </strong>
 </p>
 <p>
  <strong>
   59.说说sql的事务隔离级别
  </strong>
 </p>
 <p>
  <strong>
   60.说说mysql的存储引擎
  </strong>
 </p>
 <p>
  <strong>
   61 给你出个sql 题
  </strong>
 </p>
 <p>
  student(sid，sname,sex,class)
 </p>
 <p>
  course(cid,cname,teacher)
 </p>
 <p>
  grade(cid,sid,score)
 </p>
 <p>
  1,sex 改为age,非空，默认值为0
 </p>
 <p>
  2 统计035号课程分数大于036号课程分数的学生ID
 </p>
 <p>
  3 统计所有003班学生各门功课的课程名称和平均分
 </p>
 <p>
  <br/>
 </p>
 <p>
  以上是所有的面试题，在写sql的时候，卡了好久，因为好久没写过三表联查，子查询的sql，差不多忘了，
 </p>
 <p>
  后来下线时，一度以为自己挂了，但是过了一个多小时之后，看了一下状态，面试一轮通过，可能是面试官
 </p>
 <p>
  看我前面的问题答得还可以让我过吧。只是这问题量着实有点多。希望尽快约下轮面试。
 </p>
</div>
